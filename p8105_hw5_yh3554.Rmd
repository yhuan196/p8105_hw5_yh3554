---
title: "p8105_hw5_yh3554"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(dbplyr)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 1

First create a dataframe `full_df`that includes all data files from the directory `data/`and complete path to each file. Then use `map` over paths and read data using the `read_csv` function. 

```{r, message = FALSE, warning = FALSE}
full_df = 
  tibble(
    files = list.files("data/"),
    path = str_c("data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest(cols = c(data))
```

To tidy data dataframe need to use string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

Based on the spaghetti plots, the outcome of experiment group increase over time, but the control group not change much over time. The plots suggest high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average.


## Problem 2

#### Read the data

```{r}
homicide_df = read_csv("data_homicide/homicide-data.csv", show_col_types = FALSE)
```

#### Table of proportion of missing data

```{r}
homicide_df %>% 
  summarise_at(vars(lat:disposition), .funs = function(x) mean(is.na(x))) %>%
  knitr::kable()
```

#### Describle the raw data

The `homicide_df` is data contains homicides in 50 large U.S. It has `r nrow(homicide_df)` variables and `r ncol(homicide_df)` cases. The key variables are unique id, victim demographic information (first name, last name, age, sex), the location (city, state, latitude, longitude), and disposition. It has `r sum(is.na (homicide_df$lat))` missing latitude information and `r sum(is.na (homicide_df$lon))` missing longitude information.

#### Generate new dataframe

Create `city_state` variable by combining city and state variales. Then summarize within citries to obtain the total number of homicides and number of unsolved homicides (disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
homicide_tidy <- homicide_df %>% 
  mutate(
    city_state = str_c(city, state, sep = ", ")) %>%
  group_by(city_state) %>%
  summarize(
    homi_tot = n(),
    homi_unsolved = sum(disposition == "Closed without arrest") 
    + sum(disposition == "Open/No arrest" )
  ) %>%
  arrange(desc(homi_tot))

head(homicide_tidy)
```

#### Proportion of homicides that are unsolved for city of Baltimore

Calculate proportion of homicides that are unsolved for city of Baltimore using `prop.test` and save output as R object. Apply `broom::tidy` to pull the estimated proportion and confidence interval.

```{r}
prop_test_output <- prop.test(
  homicide_tidy %>%  filter(city_state == "Baltimore, MD") %>% pull(homi_unsolved),
  homicide_tidy %>%  filter(city_state == "Baltimore, MD") %>% pull(homi_tot))
  
save(prop_test_output, file = "prop_test_output.RData")

prop_test_output %>%
  broom::tidy() %>%
  select(estimate, conf.low, conf.high)
```

#### Proportion of homicides that are unsolved for each city

Run `prop.test` to calculate proportion of homicides that are unsolved for each city, first generate a function that 
```{r, warning = FALSE}
prop_test_all_cities <- 
  purrr::map2_df(.x = homicide_tidy$homi_unsolved,
            .y = homicide_tidy$homi_tot,
            ~broom::tidy(prop.test(.x, .y))) %>%
  select(estimate, conf.low, conf.high) %>%
  mutate(city_state = homicide_tidy$city_state) %>%
  relocate(city_state) %>%
  arrange(desc(estimate))

head(prop_test_all_cities)
```

#### Plot of estimated CI for each city

```{r}
prop_test_all_cities %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(title = "Estimate Proportion of unsloved homicides by city", 
       x = "City",
       y = "Estimate") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), 
        legend.position = "none")
```


## Problem 3

This problem will conduct a simulation to explore power of one sample t test.\
First set up the model and t test\
X ~ N($\mu$, $\sigma$)\
n = 30, $\sigma$ = 5\
$H_0$ : $\mu$ = 0     where $\alpha$ = 0.05\

```{r}
set.seed(1)
sim_fn <- function(n = 30, mu = 0, sigma = 5){
  
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma))
  
  result = sim_data %>% t.test(mu = 0, alpha = 0.05) %>%
    broom::tidy() %>%
    select(estimate, p.value)
  
  result
}

sim_fn()
```

Then generate a 5000 datasets for the model, repeat the test, and save $\hat{\mu}$ and p-value.

```{r}
sim_mu0 <- expand_grid(mu = 0, iter = 1:5000) %>% 
  mutate(
    est_df = map(.x = mu, ~sim_fn(mu=.x))) %>% 
  unnest(est_df)

head(sim_mu0)
```

Repeat the same process for $\mu$ = {1,2,3,4,5,6}\

```{r}
sim_result <- expand_grid(mu = 1:6, iter = 1:5000)  %>%
mutate(
    est_df = map(.x = mu, ~sim_fn(mu=.x))) %>% 
  unnest(est_df)

head(sim_result)
```

#### Plot of proportion of times the null was rejected

```{r}

```

